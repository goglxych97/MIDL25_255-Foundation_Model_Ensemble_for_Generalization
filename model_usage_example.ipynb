{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5675f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7599cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "#CTransPath\n",
    "#https://github.com/Xiyue-Wang/TransPath.git\n",
    "import timm\n",
    "from timm.models.layers.helpers import to_2tuple\n",
    "\n",
    "class ConvStem(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert patch_size == 4\n",
    "        assert embed_dim % 8 == 0\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.flatten = flatten\n",
    "\n",
    "\n",
    "        stem = []\n",
    "        input_dim, output_dim = 3, embed_dim // 8\n",
    "        for l in range(2):\n",
    "            stem.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=2, padding=1, bias=False))\n",
    "            stem.append(nn.BatchNorm2d(output_dim))\n",
    "            stem.append(nn.ReLU(inplace=True))\n",
    "            input_dim = output_dim\n",
    "            output_dim *= 2\n",
    "        stem.append(nn.Conv2d(input_dim, embed_dim, kernel_size=1))\n",
    "        self.proj = nn.Sequential(*stem)\n",
    "\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "def ctranspath():\n",
    "    model = timm.create_model(\"swin_tiny_patch4_window7_224\", embed_layer=ConvStem, pretrained=False)\n",
    "    return model\n",
    "\n",
    "td = torch.load(\"model_weights/CtransPath.pth\")\n",
    "model_ctranspath = ctranspath()\n",
    "model_ctranspath.head = nn.Identity()\n",
    "model_ctranspath.load_state_dict(td['model'], strict=True)\n",
    "print(model_ctranspath(torch.zeros(1,3,224,224)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573cec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1536])\n"
     ]
    }
   ],
   "source": [
    "#Prov-GigaPath\n",
    "#https://github.com/prov-gigapath/prov-gigapath.git\n",
    "import timm\n",
    "\n",
    "model_gigapath = timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=False)\n",
    "model_gigapath.load_state_dict(torch.load(\"model_weights/Prov_Gigapath.pt\"))\n",
    "print(model_gigapath(torch.zeros(1,3,224,224)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079a08ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "#PathDino\n",
    "#https://github.com/KimiaLabMayo/PathDino.git\n",
    "from PathDino import get_pathDino_model\n",
    "\n",
    "model_pathdino = get_pathDino_model(weights_path=\"model_weights/PathDino512.pth\")\n",
    "print(model_pathdino(torch.zeros(1,3,512,512)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4dc7344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "#UNI\n",
    "#https://github.com/mahmoodlab/UNI.git\n",
    "import timm\n",
    "\n",
    "model_uni = timm.create_model(\n",
    "    \"vit_large_patch16_224\",\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    init_values=1e-5,\n",
    "    num_classes=0,\n",
    "    dynamic_img_size=True,\n",
    ")\n",
    "model_uni.load_state_dict(torch.load(\"model_weights/uni.bin\", map_location=\"cpu\"), strict=True)\n",
    "print(model_uni(torch.zeros(1,3,224,224)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3983c272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 1280])\n",
      "torch.Size([1, 1280])\n"
     ]
    }
   ],
   "source": [
    "#Virchow\n",
    "#https://huggingface.co/paige-ai/Virchow\n",
    "import timm\n",
    "from timm.layers import SwiGLUPacked\n",
    "\n",
    "model_virchow = timm.create_model(\"hf-hub:paige-ai/Virchow\", pretrained=False, mlp_layer=SwiGLUPacked, act_layer=nn.SiLU)\n",
    "model_virchow.load_state_dict(torch.load(\"model_weights/virchow.pth\"))\n",
    "print(model_virchow(torch.zeros(1,3,224,224)).shape)\n",
    "print(model_virchow(torch.zeros(1,3,224,224))[:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00f3c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ./BEPH_backbone.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: backbone.mask_token, backbone.rel_pos_bias.relative_position_bias_table, backbone.rel_pos_bias.relative_position_index\n",
      "\n",
      "missing keys in source state_dict: backbone.layers.0.attn.relative_position_bias_table, backbone.layers.0.attn.relative_position_index, backbone.layers.1.attn.relative_position_bias_table, backbone.layers.1.attn.relative_position_index, backbone.layers.2.attn.relative_position_bias_table, backbone.layers.2.attn.relative_position_index, backbone.layers.3.attn.relative_position_bias_table, backbone.layers.3.attn.relative_position_index, backbone.layers.4.attn.relative_position_bias_table, backbone.layers.4.attn.relative_position_index, backbone.layers.5.attn.relative_position_bias_table, backbone.layers.5.attn.relative_position_index, backbone.layers.6.attn.relative_position_bias_table, backbone.layers.6.attn.relative_position_index, backbone.layers.7.attn.relative_position_bias_table, backbone.layers.7.attn.relative_position_index, backbone.layers.8.attn.relative_position_bias_table, backbone.layers.8.attn.relative_position_index, backbone.layers.9.attn.relative_position_bias_table, backbone.layers.9.attn.relative_position_index, backbone.layers.10.attn.relative_position_bias_table, backbone.layers.10.attn.relative_position_index, backbone.layers.11.attn.relative_position_bias_table, backbone.layers.11.attn.relative_position_index, backbone.ln2.weight, backbone.ln2.bias\n",
      "\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "#BEPH\n",
    "#https://github.com/Zhcyoung/BEPH.git\n",
    "from mmselfsup.apis import init_model\n",
    "from mmengine.config import Config\n",
    "\n",
    "cfg = Config.fromfile(\"beitv2_vit.py\")\n",
    "model_beph = init_model(cfg, \"model_weights/BEPH_backbone.pth\", device='cpu').backbone\n",
    "print(model_beph(torch.zeros(1,3,224,224))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf17cd18-1398-4833-abf6-bbecdf8e5c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "#Hibou\n",
    "#https://github.com/HistAI/hibou.git\n",
    "\n",
    "from hibou import build_model\n",
    "\n",
    "model_hibou = build_model(weights_path=\"model_weights/hibou-b.pth\")\n",
    "print(model_hibou(torch.zeros(1,3,224,224)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac8ad3b8-dcea-4ce0-ad8c-cf3e140f95b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "#HIPT\n",
    "#https://github.com/mahmoodlab/HIPT.git\n",
    "import os\n",
    "import vision_transformer as vits\n",
    "\n",
    "def get_vit256(pretrained_weights, arch='vit_small', device=torch.device('cuda:0')):\n",
    "    r\"\"\"\n",
    "    Builds ViT-256 Model.\n",
    "    \n",
    "    Args:\n",
    "    - pretrained_weights (str): Path to ViT-256 Model Checkpoint.\n",
    "    - arch (str): Which model architecture.\n",
    "    - device (torch): Torch device to save model.\n",
    "    \n",
    "    Returns:\n",
    "    - model256 (torch.nn): Initialized model.\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoint_key = 'teacher'\n",
    "    device = torch.device(\"cpu\")\n",
    "    model256 = vits.__dict__[arch](patch_size=16, num_classes=0)\n",
    "    for p in model256.parameters():\n",
    "        p.requires_grad = False\n",
    "    model256.eval()\n",
    "    model256.to(device)\n",
    "\n",
    "    if os.path.isfile(pretrained_weights):\n",
    "        state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "        if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "            print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "            state_dict = state_dict[checkpoint_key]\n",
    "        # remove `module.` prefix\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        # remove `backbone.` prefix induced by multicrop wrapper\n",
    "        state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "        msg = model256.load_state_dict(state_dict, strict=False)\n",
    "        print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))s\n",
    "        \n",
    "    return model256\n",
    "\n",
    "model_hipt = get_vit256(pretrained_weights=\"model_weights/vit256_small_dino.pth\")\n",
    "print(model_hipt(torch.zeros(1,3,256,256)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f31bb3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "#CONCH\n",
    "#https://github.com/mahmoodlab/CONCH.git\n",
    "from conch.open_clip_custom import create_model_from_pretrained\n",
    "\n",
    "model_conch, _ = create_model_from_pretrained('conch_ViT-B-16', \"hf_hub:MahmoodLab/conch\", hf_auth_token=\"_\")\n",
    "print(model_conch.encode_image(torch.zeros(1,3,448,448), proj_contrast=False, normalize=False).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd332b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "#Pathoduet\n",
    "#https://github.com/openmedlab/PathoDuet.git\n",
    "from vits import VisionTransformerMoCo\n",
    "\n",
    "model_pathoduet = VisionTransformerMoCo(pretext_token=True, global_pool='avg')\n",
    "model_pathoduet.load_state_dict(torch.load('model_weights/checkpoint_HE.pth', map_location=\"cpu\"), strict=False)\n",
    "model_pathoduet.head = nn.Identity()\n",
    "print(model_pathoduet(torch.zeros(1,3,224,224)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a337976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "#Ciga. et al.\n",
    "#https://github.com/ozanciga/self-supervised-histopathology.git\n",
    "import torchvision\n",
    "\n",
    "def load_model_weights(model, weights):\n",
    "    model_dict = model.state_dict()\n",
    "    weights = {k: v for k, v in weights.items() if k in model_dict}\n",
    "    if weights == {}:\n",
    "        print('No weight could be loaded..')\n",
    "    model_dict.update(weights)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model_ciga = torchvision.models.__dict__['resnet18'](pretrained=False)\n",
    "state_dict = torch.load('model_weights/tenpercent_resnet18.ckpt', map_location='cpu')['state_dict']\n",
    "for key in list(state_dict.keys()):\n",
    "    state_dict[key.replace('model.', '').replace('resnet.', '')] = state_dict.pop(key)\n",
    "\n",
    "model_ciga = load_model_weights(model_ciga, state_dict)\n",
    "model_ciga.fc = nn.Identity()\n",
    "print(model_ciga(torch.zeros(1,3,224,224)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14460802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-03 14:32:07.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrl_benchmarks.models.feature_extractors.ibot_vit\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mPretrained weights found at model_weights/ibot_vit_base_pancan.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v', 'head.last_layer2.weight_g', 'head.last_layer2.weight_v'])\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "#Phikon\n",
    "#https://github.com/owkin/HistoSSLscaling.git\n",
    "from rl_benchmarks.models import iBOTViT\n",
    "\n",
    "model_phikon = iBOTViT(architecture=\"vit_base_pancan\", encoder=\"teacher\", weights_path= \"model_weights/ibot_vit_base_pancan.pth\")\n",
    "\n",
    "print(model_phikon(torch.zeros(1,3,224,224)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf9823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
